//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-24817639
// Cuda compilation tools, release 10.0, V10.0.130
// Based on LLVM 3.4svn
//

.version 6.3
.target sm_30
.address_size 64

	// .globl	_Z10TestKernelPi

.visible .entry _Z10TestKernelPi(
	.param .u64 _Z10TestKernelPi_param_0
)
{
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [_Z10TestKernelPi_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	atom.global.add.u32 	%r1, [%rd2], 5;
	ret;
}

	// .globl	_Z9intrinsicPf
.visible .entry _Z9intrinsicPf( // intrinsic() kernel function. Pf stands for pointer float
	.param .u64 _Z9intrinsicPf_param_0
)
{
    // We can see the ptx code generated by intrinsic function is much less than the one generated by standard function,
    // which results in lower accuracy but higher performance.
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [_Z9intrinsicPf_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	ld.global.f32 	%f1, [%rd2];
	lg2.approx.f32 	%f2, %f1;
	mul.f32 	%f3, %f2, 0f40066666;
	ex2.approx.f32 	%f4, %f3;
	st.global.f32 	[%rd2], %f4;
	ret;
}

	// .globl	_Z8standardPf
.visible .entry _Z8standardPf( // standard() kernel function. The last Pf stands for pointer float
	.param .u64 _Z8standardPf_param_0
)
{
	.reg .pred 	%p<18>;
	.reg .f32 	%f<100>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd2, [_Z8standardPf_param_0];
	cvta.to.global.u64 	%rd1, %rd2;
	mov.f32 	%f17, 0f3F866666;
	cvt.rzi.f32.f32	%f18, %f17;
	fma.rn.f32 	%f19, %f18, 0fC0000000, 0f40066666;
	abs.f32 	%f1, %f19;
	ld.global.f32 	%f2, [%rd1];
	abs.f32 	%f3, %f2;
	setp.lt.f32	%p2, %f3, 0f00800000;
	mul.f32 	%f20, %f3, 0f4B800000;
	selp.f32	%f21, 0fC3170000, 0fC2FE0000, %p2;
	selp.f32	%f22, %f20, %f3, %p2;
	mov.b32 	 %r1, %f22;
	and.b32  	%r2, %r1, 8388607;
	or.b32  	%r3, %r2, 1065353216;
	mov.b32 	 %f23, %r3;
	shr.u32 	%r4, %r1, 23;
	cvt.rn.f32.u32	%f24, %r4;
	add.f32 	%f25, %f21, %f24;
	setp.gt.f32	%p3, %f23, 0f3FB504F3;
	mul.f32 	%f26, %f23, 0f3F000000;
	add.f32 	%f27, %f25, 0f3F800000;
	selp.f32	%f28, %f26, %f23, %p3;
	selp.f32	%f29, %f27, %f25, %p3;
	add.f32 	%f30, %f28, 0fBF800000;
	add.f32 	%f16, %f28, 0f3F800000;
	// inline asm
	rcp.approx.ftz.f32 %f15,%f16;
	// inline asm
	add.f32 	%f31, %f30, %f30;
	mul.f32 	%f32, %f15, %f31;
	mul.f32 	%f33, %f32, %f32;
	mov.f32 	%f34, 0f3C4CAF63;
	mov.f32 	%f35, 0f3B18F0FE;
	fma.rn.f32 	%f36, %f35, %f33, %f34;
	mov.f32 	%f37, 0f3DAAAABD;
	fma.rn.f32 	%f38, %f36, %f33, %f37;
	mul.rn.f32 	%f39, %f38, %f33;
	mul.rn.f32 	%f40, %f39, %f32;
	sub.f32 	%f41, %f30, %f32;
	neg.f32 	%f42, %f32;
	add.f32 	%f43, %f41, %f41;
	fma.rn.f32 	%f44, %f42, %f30, %f43;
	mul.rn.f32 	%f45, %f15, %f44;
	add.f32 	%f46, %f40, %f32;
	sub.f32 	%f47, %f32, %f46;
	add.f32 	%f48, %f40, %f47;
	add.f32 	%f49, %f45, %f48;
	add.f32 	%f50, %f46, %f49;
	sub.f32 	%f51, %f46, %f50;
	add.f32 	%f52, %f49, %f51;
	mov.f32 	%f53, 0f3F317200;
	mul.rn.f32 	%f54, %f29, %f53;
	mov.f32 	%f55, 0f35BFBE8E;
	mul.rn.f32 	%f56, %f29, %f55;
	add.f32 	%f57, %f54, %f50;
	sub.f32 	%f58, %f54, %f57;
	add.f32 	%f59, %f50, %f58;
	add.f32 	%f60, %f52, %f59;
	add.f32 	%f61, %f56, %f60;
	add.f32 	%f62, %f57, %f61;
	sub.f32 	%f63, %f57, %f62;
	add.f32 	%f64, %f61, %f63;
	mov.f32 	%f65, 0f40066666;
	mul.rn.f32 	%f66, %f65, %f62;
	neg.f32 	%f67, %f66;
	fma.rn.f32 	%f68, %f65, %f62, %f67;
	fma.rn.f32 	%f69, %f65, %f64, %f68;
	mov.f32 	%f70, 0f00000000;
	fma.rn.f32 	%f71, %f70, %f62, %f69;
	add.rn.f32 	%f72, %f66, %f71;
	neg.f32 	%f73, %f72;
	add.rn.f32 	%f74, %f66, %f73;
	add.rn.f32 	%f75, %f74, %f71;
	mov.b32 	 %r5, %f72;
	setp.eq.s32	%p4, %r5, 1118925336;
	add.s32 	%r6, %r5, -1;
	mov.b32 	 %f76, %r6;
	add.f32 	%f77, %f75, 0f37000000;
	selp.f32	%f78, %f76, %f72, %p4;
	selp.f32	%f4, %f77, %f75, %p4;
	mul.f32 	%f79, %f78, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f80, %f79;
	mov.f32 	%f81, 0fBF317200;
	fma.rn.f32 	%f82, %f80, %f81, %f78;
	mov.f32 	%f83, 0fB5BFBE8E;
	fma.rn.f32 	%f84, %f80, %f83, %f82;
	mul.f32 	%f85, %f84, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f86, %f85;
	add.f32 	%f87, %f80, 0f00000000;
	ex2.approx.f32 	%f88, %f87;
	mul.f32 	%f89, %f86, %f88;
	setp.lt.f32	%p5, %f78, 0fC2D20000;
	selp.f32	%f90, 0f00000000, %f89, %p5;
	setp.gt.f32	%p6, %f78, 0f42D20000;
	selp.f32	%f97, 0f7F800000, %f90, %p6;
	setp.eq.f32	%p7, %f97, 0f7F800000;
	@%p7 bra 	BB2_2;

	fma.rn.f32 	%f97, %f97, %f4, %f97;

BB2_2:
	setp.lt.f32	%p8, %f2, 0f00000000;
	setp.eq.f32	%p9, %f1, 0f3F800000;
	and.pred  	%p1, %p8, %p9;
	mov.b32 	 %r7, %f97;
	xor.b32  	%r8, %r7, -2147483648;
	mov.b32 	 %f91, %r8;
	selp.f32	%f99, %f91, %f97, %p1;
	setp.eq.f32	%p10, %f2, 0f00000000;
	@%p10 bra 	BB2_5;
	bra.uni 	BB2_3;

BB2_5:
	add.f32 	%f94, %f2, %f2;
	selp.f32	%f99, %f94, 0f00000000, %p9;
	bra.uni 	BB2_6;

BB2_3:
	setp.geu.f32	%p11, %f2, 0f00000000;
	@%p11 bra 	BB2_6;

	cvt.rzi.f32.f32	%f93, %f65;
	setp.neu.f32	%p12, %f93, 0f40066666;
	selp.f32	%f99, 0f7FFFFFFF, %f99, %p12;

BB2_6:
	add.f32 	%f95, %f3, 0f40066666;
	mov.b32 	 %r9, %f95;
	setp.lt.s32	%p14, %r9, 2139095040;
	@%p14 bra 	BB2_11;

	setp.gtu.f32	%p15, %f3, 0f7F800000;
	@%p15 bra 	BB2_10;
	bra.uni 	BB2_8;

BB2_10:
	add.f32 	%f99, %f2, 0f40066666;
	bra.uni 	BB2_11;

BB2_8:
	setp.neu.f32	%p16, %f3, 0f7F800000;
	@%p16 bra 	BB2_11;

	selp.f32	%f99, 0fFF800000, 0f7F800000, %p1;

BB2_11:
	setp.eq.f32	%p17, %f2, 0f3F800000;
	selp.f32	%f96, 0f3F800000, %f99, %p17;
	st.global.f32 	[%rd1], %f96;
	ret;
}


